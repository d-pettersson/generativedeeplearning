{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# GPU related import options - for watching real-time memory use (in console): watch nvidia-smi\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the structure of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = (28, 28, 1)\n",
    "encoder_conv_filters = [32,64,64,64]\n",
    "encoder_conv_kernel_size = [3,3,3,3]\n",
    "encoder_conv_strides = [1,2,2,1]\n",
    "decoder_conv_t_filters = [64,64,32,1]\n",
    "decoder_conv_t_kernel_size = [3,3,3,3]\n",
    "decoder_conv_t_strides = [1,2,2,1]\n",
    "z_dim = 2\n",
    "\n",
    "use_batch_norm = False\n",
    "use_dropout = False\n",
    "\n",
    "n_layers_encoder = len(encoder_conv_filters)\n",
    "n_layers_decoder = len(decoder_conv_t_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input to the encoder (the image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape = input_dim, name = 'encoder_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder_input\n",
    "\n",
    "for i in range(n_layers_encoder):\n",
    "    conv_layer = Conv2D(\n",
    "        filters = encoder_conv_filters[i],\n",
    "        kernel_size = encoder_conv_kernel_size[i],\n",
    "        strides = encoder_conv_strides[i],\n",
    "        padding = 'same',\n",
    "        name = 'encoder_conv_' + str(i))\n",
    "    \n",
    "    x = conv_layer(x) # Stack the convolutional layers on top of each other\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    if use_dropout:\n",
    "        x = Dropout(rate = 0.25)(x)\n",
    "\n",
    "shape_before_flattening = K.int_shape(x)[1:]\n",
    "\n",
    "x = Flatten()(x) # Flatten the last conv. layer to a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layer that connects the vector to the 2D latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = Dense(z_dim, name = 'encoder_output')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras model defining the encoder - a model that takes an input image and encodes it into the 2D latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(encoder_input, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)      (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)      (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)      (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "encoder_output (Dense)       (None, 2)                 6274      \n",
      "=================================================================\n",
      "Total params: 98,946\n",
      "Trainable params: 98,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the input to the decoder (the point in the latent space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape = (z_dim, ), name = 'decoder_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect the input to a Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(np.prod(shape_before_flattening))(decoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the vector into a tensor that can be fed as input into the first conv. transpose layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Reshape(shape_before_flattening)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_layers_decoder):\n",
    "    conv_t_layer = Conv2DTranspose(\n",
    "        filters = decoder_conv_t_filters[i],\n",
    "        kernel_size = decoder_conv_t_kernel_size[i],\n",
    "        strides = decoder_conv_t_strides[i],\n",
    "        padding = 'same',\n",
    "        name = 'decoder_conv_t_' + str(i))\n",
    "    \n",
    "    x = conv_t_layer(x) # Stack the conv. transpose layers on top of each other\n",
    "    \n",
    "    if i < n_layers_decoder - 1:\n",
    "        x = LeakyReLU()(x)\n",
    "        \n",
    "        if use_batch_norm:\n",
    "            x = BatchNormalization()(x)\n",
    "        if use_dropout:\n",
    "            x = Dropout(rate = 0.25)(x)\n",
    "    else:\n",
    "        x = Activation('sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras model defining the decoder - a model that takes a point in the latent space and decodes it into the original image domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Model(decoder_input, decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_t_0 (Conv2DTran (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_t_1 (Conv2DTran (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_t_2 (Conv2DTran (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_t_3 (Conv2DTran (None, 28, 28, 1)         289       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 102,017\n",
      "Trainable params: 102,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the Encoder to the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = encoder_input # The input to the autoencoder is the same as the input to the encoder\n",
    "model_output = decoder(encoder_output) # The output from the autoencoder is the output from the encoder passed through the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras model defining the full autoencoder - a model taking a image, passing it through the encoder and back out through the decoder to generate a reconstruction of the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "batch_size = 32\n",
    "initial_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr = learning_rate)\n",
    "\n",
    "def r_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1, 2, 3])\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = r_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0551\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0462\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0442\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.0431\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0424\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0419\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0415\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0412\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0409\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0407\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0405\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0403\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0401\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0400\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0399\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.0397\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0396\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0396\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0395\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0393\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0393\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.0392\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0391\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0390\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0390\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.0389\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0388\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0388\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0387\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0387\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0386\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0385\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0385\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0385\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0384\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0384\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0383\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0383\n",
      "Epoch 39/200\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.0382\n",
      "Epoch 40/200\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0382\n",
      "Epoch 41/200\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0382\n",
      "Epoch 42/200\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.0381\n",
      "Epoch 43/200\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0380\n",
      "Epoch 44/200\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 0.0380\n",
      "Epoch 45/200\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0380\n",
      "Epoch 46/200\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0380\n",
      "Epoch 47/200\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.0379\n",
      "Epoch 48/200\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0379\n",
      "Epoch 49/200\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 0.0379\n",
      "Epoch 50/200\n",
      "53472/60000 [=========================>....] - ETA: 1s - loss: 0.0378"
     ]
    }
   ],
   "source": [
    "model.fit(x = x_train,\n",
    "          y = x_train,\n",
    "          batch_size = batch_size,\n",
    "          shuffle = True,\n",
    "          epochs = 200,\n",
    "          initial_epoch = initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
